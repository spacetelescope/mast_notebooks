{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "<!-- # Pan-STARRS access using MAST's TAP service: How to chunk large / long queries -->\n",
    "# Accessing MAST TAP services: How to chunk large / long queries\n",
    "\n",
    "******\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to split up a large query (in terms of total number of rows returned or total execution time) for submission to Table Access Protocol (TAP) services at MAST. The examples presented here access the Pan-STARRS TAP service.\n",
    "\n",
    "This includes how to scope out the query execution time and \"row density\" (number of rows for a given ID or spatial range), how to scale to appropriately-sized query chunks given the TAP service limitations, and how to structure the chunks with parallel and serial components (to avoid overusing database resources with too many queries in parallel).\n",
    "\n",
    "## Learning Goals\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "- Understand how to **estimate the scaling of execution time and numbers of rows returned** with ID range or sky coverage\n",
    "- Understand how to **use the scaling estimation to design full query chunks** that are compatible with TAP service limitations\n",
    "- Understand how to submit the chunked queries in a manner that **does not overuse** database resources.\n",
    "\n",
    "****\n",
    "### Table of Contents\n",
    "\n",
    "1. [Imports](#Imports)\n",
    "1. [Connect to TAP service](#Connect-to-TAP-service)\n",
    "1. [Too many rows](#Too-many-rows)\n",
    "1. [Long query duration](#Long-query-duration)\n",
    "1. [Additional Resources](#Additional-Resources)\n",
    "1. [About This Notebook](#About-this-Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*******\n",
    "## Introduction\n",
    "\n",
    "Welcome! This notebook demonstrates how to perform large queries accessing the Pan-STARRS catalogs at MAST through a Virual Observatory Table Access Protocol (TAP) service, where **the full query has:**\n",
    "1. **an execution time _longer_ than the time limit** for queries to the TAP service, and/or\n",
    "2. **_more_ result rows than the maximum size of rows** returned by the TAP service.\n",
    "\n",
    "In this tutorial, the example queries are drawn from the Pan-STARRS \"20 queries\" tutorials. Please see <b style=\"color:red\">the 20 queries notebook [LINK HERE]</b> for full information about these queries.\n",
    "\n",
    "The workflow for this notebook consists of:\n",
    "* [Imports](#Imports)\n",
    "* [Connect to TAP service](#Connect-to-TAP-service)\n",
    "    * [TAP service limits](#TAP-service-limits)\n",
    "* [Too many rows](#Too-many-rows)\n",
    "    * [Determining the query row/time scaling](#Q2-query-scaling)\n",
    "    * [Submitting the partitioned query](#Q2-submitting-query)\n",
    "* [Long query duration](#Long-query-duration)\n",
    "    * [Determining the query row/time scaling](#Q4-query-scaling)\n",
    "    * [Submitting the partitioned query](#Q4-submitting-query)\n",
    "* [Additional Resources](#Additional-Resources)\n",
    "* [About This Notebook](#About-this-Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imports\n",
    "This tutorial makes use of the following libraries: \n",
    "- [*numpy*](https://numpy.org/) for numerical calculations\n",
    "- [*pyvo*](https://pyvo.readthedocs.io) for querying the MAST catalogs via TAP\n",
    "- *time*, *datetime* to determine query duration\n",
    "- [*astropy.table vstack*](https://docs.astropy.org/en/stable/table/index.html) to concatenate chunk result tables\n",
    "- [*astropy.conf*](https://docs.astropy.org/en/stable/config/index.html) to modify number of lines displayed for tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyvo as vo\n",
    "import datetime\n",
    "import time\n",
    "from astropy.table import vstack\n",
    "\n",
    "from astropy import conf\n",
    "conf.max_lines = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## Connect to TAP service\n",
    "\n",
    "For all queries, we will be connecting to the Pan-STARRS (PS1) Data release 2 (DR2) catalog. Specifically, we will connect to the new [PS1 DR2 postgres-backed TAP service](https://mast.stsci.edu/vo-tap/api/v0.1/ps1_dr2/), which offers improved performance relative to the legacy database (by factors of 100 or greater, in many cases).\n",
    "\n",
    "See the [PS1 documentation](link_to_migration_guide_here) for information about the tables available with this new TAP service.\n",
    "    \n",
    "<div class=\"alert alert-danger\">\n",
    "<b>FIX LINK TO MIGRATION GUIDE ABOVE</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAP_service = vo.dal.TAPService(\n",
    "    \"https://mast.stsci.edu/vo-tap/api/v0.1/ps1_dr2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAP service limits\n",
    "\n",
    "This service supports the following ADQL features, \n",
    "and will return up to **100,000 rows** --- which can be found in the TAP service description as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAP_service.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum query execution duration for this TAP service is **1200 seconds** (20 min).  The default job execution duration is 600 seconds.\n",
    "\n",
    "*This can be found as follows:*\n",
    "1. *Create a job with an empty query.*\n",
    "2. *Set the execution duration to a very long time.*\n",
    "3. *Check the job execution duration (set to the higher of the specified duration or the service maximum)*\n",
    "4. *Delete the job*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = TAP_service.submit_job(\"\")\n",
    "job.execution_duration = int(1e7)   # seconds; Almost 1/3 of a year\n",
    "print(job.execution_duration)\n",
    "job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    "*********\n",
    "\n",
    "## Too many rows\n",
    "\n",
    "First, we will examine Q2 from <b style=\"color:red\">the Pan-STARRS \"20 queries\" [add link here]</b>, which looks for galaxies with a specified blue surface brightness within a fairly large swath of the sky (RA between 170 and 190 deg, and Decl. less than 0).\n",
    "\n",
    "The full query is written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_adql_query = f\"\"\"\n",
    "SELECT fmo.objID, fmo.gFKronMag,\n",
    "   fmo.gFKronMag + 2.5*log10(PI()*POWER(so.gKronRad,2)) AS bsurfmag, \n",
    "   fmo.raMean, fmo.decMean\n",
    "FROM forced_mean_object as fmo\n",
    "JOIN stack_object AS so ON \n",
    "    fmo.objID=so.objID\n",
    "WHERE ((fmo.gFKronMag > 0) AND\n",
    "       (fmo.gFPSFMag - fmo.gFKronMag > 0.05)) -- galaxies\n",
    "AND fmo.raMean BETWEEN 170 AND 190            -- ra substitute for super-gal coords\n",
    "AND fmo.decMean < 0\n",
    "AND fmo.gFKronMag + 2.5*log10(PI()*POWER(so.gKronRad,2))\n",
    "       BETWEEN 23 AND 25                      -- mag per sq arcsec\n",
    "AND so.primaryDetection = 1                   -- primary detection in stack_object\"\"\"\n",
    "print(full_adql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "<a id=\"Q2-query-scaling\"></a>\n",
    "### Determining the query row/time scaling\n",
    "\n",
    "First, we need submit a test query with a restricted range in order to estimate how the query execution time and number of rows scales with ID range (or area).\n",
    "\n",
    "To do this, we will place limits on the `objID`. \n",
    "\n",
    "We will leverage the fact that [Pan-STARRS object identifiers](https://outerspace.stsci.edu/spaces/Pan-STARRS/pages/298812384/PS1+Object+Identifiers) are determined from their position on the sky to determine an `objID` range. Since the first 5 digits of `objID` are determined from `floor((decl+90)/0.00833333)`, we can easily determine objID ranges from a declination range.\n",
    "\n",
    "For this first test query, we will limit `objID` based on a declination range of -5$^{\\circ}$$\\leq$decMean$<$0$^{\\circ}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_objID_from_decl(decl):\n",
    "    return np.int64(np.floor((decl+90)/0.00833333)*1e13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decl_range = [-5, 0]\n",
    "objidrange = [get_objID_from_decl(decl) for decl in decl_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adql_query = f\"\"\"{full_adql_query}\n",
    "AND fmo.objID >= {objidrange[0]}           -- dec range subselection\n",
    "AND fmo.objID < {objidrange[1]}\"\"\"\n",
    "print(test_adql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "job = TAP_service.run_async(test_adql_query)\n",
    "end = time.time()\n",
    "print(f\"Elapsed time: {str(datetime.timedelta(seconds=end-start))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAP_results = job.to_table()\n",
    "TAP_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the result table has *100,000 rows*, equal to the service maximum limit.**\n",
    "\n",
    "It is almost certain that the results were truncated.\n",
    "\n",
    "Thus, we will submit a second test query with a smaller declination limit range of -1$^{\\circ}$$\\leq$decMean$<$0$^{\\circ}$.\n",
    "\n",
    "*(The test query execution time is well under the service maximum, and thus not a concern for our partitioning).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decl_range = [-1, 0]\n",
    "objidrange = [get_objID_from_decl(decl) for decl in decl_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adql_query = f\"\"\"{full_adql_query}\n",
    "AND fmo.objID >= {objidrange[0]}           -- dec range subselection\n",
    "AND fmo.objID < {objidrange[1]}\"\"\"\n",
    "print(test_adql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "job = TAP_service.run_async(test_adql_query)\n",
    "end = time.time()\n",
    "print(f\"Elapsed time: {str(datetime.timedelta(seconds=end-start))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAP_results = job.to_table()\n",
    "TAP_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second test query has fewer results than the maximum, indicating we have received all matching results.\n",
    "\n",
    "Since the area of a 1 degree range of declination, for the same RA range, decreases away from the equator, all other 1 degree declination chunks from -30 deg to 0 deg likely have fewer objects. *(Though in general, the number of results will also scale with source density --- a concern for high density regions such as the galactic plane.)*\n",
    "\n",
    "So in this case, we will proceed with **declination range chunks of [-30,-29), [-29,-28), ..., [-2, -1), [-1, 0.01).**  Note we slightly pad the upper limit (to account for the remaining digits in the objIDs), ensuring the objects very close to decl. ~ 0 deg are included.\n",
    "\n",
    "----\n",
    "<a id=\"Q2-submitting-query\"></a>\n",
    "### Submitting the partitioned query\n",
    "\n",
    "With the query chunking scheme defined, we next proceed to submit the partitioned queries.\n",
    "\n",
    "**Submitting all chunks in parallel (simultaneously) will overuse database resources.**  \n",
    "<!-- Simultaneous queries may result in IP blocking, in order to ensure other users can access the database. -->\n",
    "\n",
    "It is acceptable to submit a few (**$\\boldsymbol{\\sim}$5**) simultaneous queries to MAST's TAP services.\n",
    "\n",
    "Thus, for our example query, we will structure our job submission as follows:\n",
    "1. Generate all 1 deg. declination range query ADQL strings.\n",
    "2. Loop over the full set of chunked queries: submit 5 queries simultaneously, wait for these to complete, and gather the results; the proceed to the next simultaneous set of queries.\n",
    "\n",
    "As part of this process, we well also check that each query chunk returns FEWER than the maximum number of rows (to ensure no results are lost), and that each query chunk successfully executed.\n",
    "\n",
    "A short helper function, defined below, will handle the process of submitting simultaneous queries and gathering the results, including checking for errors or potentially missing results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run simultaneous queries and gather the results:\n",
    "def run_simultaneous_queries(\n",
    "    queries, tap_service,\n",
    "    sleep_interval_sec=10, maximum_rows=100000,\n",
    "    execution_duration=None\n",
    "):\n",
    "    # Start simultaneous jobs:\n",
    "    jobs = []\n",
    "    for query in queries:\n",
    "        # Initialize job\n",
    "        job = tap_service.submit_job(query)\n",
    "\n",
    "        # Set execution time, if specified:\n",
    "        if execution_duration is not None:\n",
    "            job.execution_duration = execution_duration\n",
    "\n",
    "        # Run job and add to list:\n",
    "        job.run()\n",
    "        jobs.append(job)\n",
    "\n",
    "    # Wait for all jobs to be finished. \n",
    "    # Based on pyvo AsyncTAPJob.wait()\n",
    "    # https://pyvo.readthedocs.io/en/latest/_modules/pyvo/dal/tap.html#AsyncTAPJob.wait\n",
    "    # Active phases:\n",
    "    active_phases = {\"QUEUED\", \"EXECUTING\", \"RUN\", \"COMPLETED\", \"ERROR\", \"UNKNOWN\"}\n",
    "    completed_phases = {\"COMPLETED\"}\n",
    "    failed_phases = {\"ABORTED\", \"ERROR\"}\n",
    "\n",
    "    while True:\n",
    "        # Check all job phases:\n",
    "        statuses = []\n",
    "        for i, job in enumerate(jobs):\n",
    "            job_phase = job.phase\n",
    "            if job_phase not in active_phases:\n",
    "                raise ValueError(\n",
    "                    \"Job no longer active!\\n\"\n",
    "                    f\"URL: {job.url}\\n\"\n",
    "                    \"Query:\\n\"\n",
    "                    f\"{job.query}\"\n",
    "                )\n",
    "\n",
    "            if job_phase in failed_phases:\n",
    "                raise ValueError(\n",
    "                    f\"Job status: {job_phase}\\n\"\n",
    "                    f\"URL: {job.url}\\n\"\n",
    "                    \"Query:\\n\"\n",
    "                    f\"{job.query}\"\n",
    "                )\n",
    "\n",
    "            statuses.append(True if job_phase in completed_phases else False)\n",
    "\n",
    "        # Break if all jobs are completed:\n",
    "        if np.all(statuses):\n",
    "            break\n",
    "\n",
    "        # Otherwise, pause until next check\n",
    "        time.sleep(sleep_interval_sec)\n",
    "\n",
    "    # Collect results, and check that no query hit the maximum number of results\n",
    "    results = []\n",
    "    for job in jobs:\n",
    "        result = job.fetch_result().to_table()\n",
    "        if len(result) == maximum_rows:\n",
    "            raise ValueError(\n",
    "                \"Query hit maximum number of results!\\n\"\n",
    "                f\"URL: {job.url}\\n\"\n",
    "                \"Query:\\n\"\n",
    "                f\"{job.query}\"\n",
    "            )\n",
    "        results.append(result)\n",
    "\n",
    "        # Delete the job:\n",
    "        job.delete()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the full set of query chunks, with 1 deg. declination ranges:\n",
    "\n",
    "full_decl_range = [-30, 0]\n",
    "decl_step = 1\n",
    "\n",
    "# Generate an array with declination values on the boundaries:\n",
    "decl_bounds = np.arange(full_decl_range[0], full_decl_range[1]+decl_step, decl_step)\n",
    "# List comprehension to make lists for each decl. chunk range:\n",
    "decl_ranges = [[decl_bounds[i],decl_bounds[i+1]] for i in range(len(decl_bounds)-1)]\n",
    "\n",
    "# Pad the upper limit on the last range, to account for other digits in the objIDs \n",
    "# for objects at decl = 0 deg.\n",
    "decl_ranges[-1][1] = 0.01\n",
    "\n",
    "# Generate chunk queries\n",
    "queries = []\n",
    "for decl_range in decl_ranges:\n",
    "    objidrange = [get_objID_from_decl(decl) for decl in decl_range]\n",
    "    chunk_adql_query = f\"\"\"{full_adql_query}\n",
    "        AND fmo.objID >= {objidrange[0]}           -- dec range subselection\n",
    "        AND fmo.objID < {objidrange[1]}\"\"\"\n",
    "    queries.append(chunk_adql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit all query chunks and gather results:\n",
    "\n",
    "n_per_chunk = 5  # Number of queries run simultaneously\n",
    "n_queries = len(queries)\n",
    "results_all = []\n",
    "\n",
    "# Get wall-time to run full set of query chunks\n",
    "start = time.time()\n",
    "for i in range(int(np.ceil(n_queries/n_per_chunk))):\n",
    "    imin = i*n_per_chunk\n",
    "    imax = min([n_queries, (i+1)*n_per_chunk])\n",
    "    results = run_simultaneous_queries(\n",
    "        queries[imin:imax], TAP_service,\n",
    "        sleep_interval_sec=10, maximum_rows=100000\n",
    "    )\n",
    "    results_all.extend(results)\n",
    "end = time.time()\n",
    "print(f\"Elapsed time for full query: {str(datetime.timedelta(seconds=end-start))}\")\n",
    "\n",
    "# Aggregate all results into one table:\n",
    "# Concatenate individual tables:\n",
    "TAP_results = vstack(results_all)\n",
    "\n",
    "# Clear memory:\n",
    "results_all = None\n",
    "results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAP_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By submitting 5 query chunks at a time, it takes about **8 minutes** to run all partitions of the full query, \n",
    "obtaining the full result set of about **1.07 million objects**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b><u>Tip:</u> <i>Ensuring robustness against individual query chunk failures</i></b>\n",
    "\n",
    "Depending on the complexity and total time required to run a query, it would be beneficial to modify the above approach to \n",
    "ensure that results from successfully executed chunks are not lost in the event that another chunk query fails (due to, e.g., isolated timeout issues).\n",
    "\n",
    "This could be accomplished by saving the individual chunk results to disk, and then only rerunning those chunks where the query was previously unsuccessful.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    "*********\n",
    "\n",
    "## Long query duration\n",
    "\n",
    "Next, we will consider Q4 from <b style=\"color:red\">the Pan-STARRS \"20 queries\" [add link here]</b>, which looks for galaxies with large galaxies with large red surface brightnes and high ellipticity.\n",
    "\n",
    "The full query is written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_adql_query = \"\"\"\n",
    "SELECT so.objID, so.raMean, so.decMean, so.rKronRad, so.rKronMag,\n",
    "   so.rKronMag + 2.5*log10(PI()*POWER(so.rKronRad,2)) as rsurfmag,\n",
    "   smf.rSerAb, smf.rSerChisq,\n",
    "   sqrt(1-power(smf.rSerAb,2)) as ellipticity\n",
    "FROM stack_model_fit_ser AS smf\n",
    "JOIN stack_object AS so ON \n",
    "    smf.objID=so.objID \n",
    "    AND smf.uniquePspsSTid=so.uniquePspsSTid\n",
    "WHERE so.rKronRad BETWEEN 30 AND 60\n",
    "  AND so.rKronMag > 0\n",
    "  AND so.rKronMag + 2.5*log10(PI()*POWER(so.rKronRad,2)) < 24 -- mag per sq arcsec\n",
    "  AND smf.rSerChisq > 0\n",
    "  AND smf.rSerAb < sqrt(0.75)\n",
    "  AND so.rpsfQfPerfect > 0.9\n",
    "  AND so.nDetections > 3\n",
    "  AND so.nr > 1\"\"\"\n",
    "print(full_adql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "<a id=\"Q4-query-scaling\"></a>\n",
    "### Determining the query row/time scaling\n",
    "\n",
    "As before, we will submit a test query with a restricted range in order to estimate how the query execution time and number of rows scales.\n",
    "\n",
    "Again, we will limit the `objID` range in this test query.\n",
    "\n",
    "Since the first 5 digits of Pan-STARRS `objID` are determined by `floor((decl+90)/0.00833333)` (see the [Pan-STARRS object identifiers](https://outerspace.stsci.edu/spaces/Pan-STARRS/pages/298812384/PS1+Object+Identifiers) documentation), Pan-STARRS `objID`s fall within the estimated range 72000000000000000 to 216010000000000000 (using declination -30 to 90.01 deg, with padding to include the remaining objID digits --- thus ensuring all objects within -30 to 90 deg are included).\n",
    "\n",
    "For this test query, we will restrict the `objID`s to between 129500000000000000 and 133500000000000000 (roughly 1/36th of the estimated full range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adql_query = f\"\"\"{full_adql_query}\n",
    "  AND so.objID >= 129500000000000000     -- objID range subselection\n",
    "  AND so.objID < 133500000000000000\"\"\"\n",
    "print(test_adql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "job = TAP_service.run_async(test_adql_query)\n",
    "end = time.time()\n",
    "print(f\"Elapsed time: {str(datetime.timedelta(seconds=end-start))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAP_results = job.to_table()\n",
    "TAP_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test query takes about 40 seconds and returns 67 rows.\n",
    "\n",
    "*(As this test query has fewer than the maximum number of rows, no results were omitted and we can thus use this to determine the query scaling.)*\n",
    "\n",
    "Thus, executing the query over the full Pan-STARRS catalog (~36 times larger) would take about **24 minutes** and return about **2100 rows**.\n",
    "\n",
    "While the estimated total is within the TAP service row limit, this full query **would take longer than the service time limit**.\n",
    "\n",
    "Thus, we will split the full query in half, with **`objID` chunks of [72000000000000000, 144005000000000000), [144005000000000000, 216010000000000000)**\n",
    "\n",
    "----\n",
    "<a id=\"Q4-submitting-query\"></a>\n",
    "### Submitting the partitioned query\n",
    "\n",
    "As before, we proceed to generate and submit the partitioned queries.\n",
    "\n",
    "Since we will only use 2 chunks in this case, we will submit both simulaneously (because up to 5 parallel queries is reasonable given database resources).\n",
    "\n",
    "However, we need to **increase the the execution duration to its maximum of 20 minutes**, to cover the estimated ~12 minute execution time per chunk.\n",
    "\n",
    "We will use the previously-defined helper function to handle job submission, result gathering, and error checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the full set of query chunks, with the two objID ranges:\n",
    "objID_ranges = [\n",
    "    [72000000000000000, 144005000000000000],\n",
    "    [144005000000000000, 216010000000000000]\n",
    "    ]\n",
    "# Generate chunk queries\n",
    "queries = []\n",
    "for objidrange in objID_ranges:\n",
    "    chunk_adql_query = f\"\"\"{full_adql_query}\n",
    "  AND so.objID >= {objidrange[0]}     -- objID range subselection\n",
    "  AND so.objID < {objidrange[1]}\"\"\"\n",
    "    queries.append(chunk_adql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit all query chunks and gather results:\n",
    "\n",
    "n_per_chunk = 2  # Number of queries run simultaneously\n",
    "n_queries = len(queries)\n",
    "results_all = []\n",
    "\n",
    "# Get wall-time to run full set of query chunks\n",
    "start = time.time()\n",
    "for i in range(int(np.ceil(n_queries/n_per_chunk))):\n",
    "    imin = i*n_per_chunk\n",
    "    imax = min([n_queries, (i+1)*n_per_chunk])\n",
    "    results = run_simultaneous_queries(\n",
    "        queries[imin:imax], TAP_service,\n",
    "        sleep_interval_sec=10, maximum_rows=100000,\n",
    "        execution_duration=1200\n",
    "    )\n",
    "    results_all.extend(results)\n",
    "end = time.time()\n",
    "print(f\"Elapsed time for full query: {str(datetime.timedelta(seconds=end-start))}\")\n",
    "\n",
    "# Aggregate all results into one table:\n",
    "# Concatenate individual tables:\n",
    "TAP_results = vstack(results_all)\n",
    "\n",
    "# Clear memory:\n",
    "results_all = None\n",
    "results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAP_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By submitting the 2 query chunks simultaneously, it takes about **6.5 minutes** to run both partitions of the full query, \n",
    "obtaining the full result set of **1745 objects**.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b><u>Note:</u> <i>Intrinsic source density variations</i></b>\n",
    "\n",
    "In this case, the full result set size is only about 80% of what we estimated, suggesting our test query selected objects in an ID range (equivalently, a portion of the sky) with a higher than average source density.\n",
    "\n",
    "If the source density is known to be variable (with many selected objects in e.g., the galactic plane or nearby galaxies), submitting a test query covering a high density region/ID range will provide a conservative scaling estimate.  This will ensure none of the chunks go over the TAP service time/row limits.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "### Another chunking option for Pan-STARRS: `randomID`\n",
    "Another option available for the Pan-STARRS catalogs (though not generally for other survey's catalogs) is to chunk \n",
    "queries by ranges of `randomID` from the ObjectThin table (and joins thereof). `randomID` is a random number between 0 and 1 assigned to each object, regardless of position on the sky.  \n",
    "\n",
    "However, this tutorial presented approaches using object ID ranges (extensible to position ranges) to provide broader applicability to queries with other datasets.\n",
    "\n",
    "### Table Access Protocol\n",
    "\n",
    "- IVOA standard for RESTful web service access to tabular data\n",
    "- http://www.ivoa.net/documents/TAP/\n",
    "\n",
    "### Pan-STARRS 1 DR 2\n",
    "\n",
    "- https://outerspace.stsci.edu/display/Pan-STARRS/\n",
    "\n",
    "### Astronomical Query Data Language (2.0)\n",
    "\n",
    "- IVOA standard for querying astronomical data in tabular format, with geometric search support\n",
    "- http://www.ivoa.net/documents/latest/ADQL.html\n",
    "\n",
    "### PyVO\n",
    "\n",
    "- an affiliated package for [astropy](https://www.astropy.org/)\n",
    "- find and retrieve astronomical data available from archives that support standard IVOA virtual observatory service protocols.\n",
    "- https://pyvo.readthedocs.io/en/latest/index.html\n",
    "\n",
    "\n",
    "### Full list of MAST/TAP services\n",
    "- A full list of available MAST TAP services can be found at:\n",
    "- https://mast.stsci.edu/vo-tap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "If you use `astropy` for published research, please cite the\n",
    "authors. Follow these links for more information about citing `astropy`:\n",
    "\n",
    "* [Citing `astropy`](https://www.astropy.org/acknowledging.html)\n",
    "\n",
    "If you use Pan-STARRS data accessed through MAST for published research, \n",
    "please include the following acknowledgements, found at the following links:\n",
    "\n",
    "* [Acknowledging Pan-STARRS](https://archive.stsci.edu/publishing/mission-acknowledgements#section-895d38a0-86b3-4143-b521-6cc3312701f9)\n",
    "* [Acknowledging MAST](https://archive.stsci.edu/gsc/mast_data_use.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About this Notebook\n",
    "\n",
    "**Author(s):**  Sedona Price<br>\n",
    "**Keyword(s):** Tutorial, TAP, pyvo, ADQL, Pan-STARRS <br>\n",
    "**First Published:** 2026-01-07 <br>\n",
    "**Last Updated:** 2026-01-07\n",
    "***\n",
    "[Top of Page](#top)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/style-guides/master/guides/images/stsci-logo.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
